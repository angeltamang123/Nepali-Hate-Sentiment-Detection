{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dbc00a9-5d09-4470-a6aa-ad07a55ceb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "import numpy as np # for scientific computing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903b9dd5-d110-471a-87ab-57b3c133b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../Data/Raw/Nepali_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea44fff-995e-47ea-bc83-5945efc758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a18142-3d64-40a3-8aff-684e2aff38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom python script to transform Hate sentiment dataset's Output classes to single target class\n",
    "from create_target_label import create_target_column\n",
    "\n",
    "df_copy,label_encoder = create_target_column(df_copy, \"Sentiment\", \"Polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0cf06a-17df-4551-849e-86c158708942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FEEDBACK_0',\n",
       " 'FEEDBACK_1',\n",
       " 'GENERAL_0',\n",
       " 'GENERAL_1',\n",
       " 'PROFANITY_0',\n",
       " 'PROFANITY_1',\n",
       " 'VIOLENCE_0',\n",
       " 'VIOLENCE_1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44406f3-b394-4f34-9151-9bd50480faee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Aspect Term</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...</td>\n",
       "      <td>जोगाउन को लागि</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...</td>\n",
       "      <td>लखेटनु पछ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !</td>\n",
       "      <td>ससकृती ध्वस्त पार्ने</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...</td>\n",
       "      <td>भुमाफिया</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...</td>\n",
       "      <td>बेची सके</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Aspect Term  \\\n",
       "0  गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...        जोगाउन को लागि   \n",
       "1  दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...             लखेटनु पछ   \n",
       "2        नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !  ससकृती ध्वस्त पार्ने   \n",
       "3  मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...              भुमाफिया   \n",
       "4  नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...              बेची सके   \n",
       "\n",
       "   Target  \n",
       "0       2  \n",
       "1       3  \n",
       "2       3  \n",
       "3       3  \n",
       "4       3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8e3b1-92be-4184-a53c-221a8650aca1",
   "metadata": {},
   "source": [
    "## We created a single target for each Sentiment polarity to simplify Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f6aa87-4378-434b-8cbe-1664e6974564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Aspect Term feature\n",
    "\n",
    "df_copy.drop(\"Aspect Term\", axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739b9f5-7b2d-424e-824d-18158c76b0d0",
   "metadata": {},
   "source": [
    "## Now we import a custom python script for returning the embeddings to the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66481d-24f9-4714-8561-d8b19be69752",
   "metadata": {},
   "source": [
    "The script imported below supports dataset's embeddings generation by word2vec, fasttext, glove while also supports loading the model by simply passing pre-trained embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aed4d-74ab-443d-a4a8-b07c0c61f989",
   "metadata": {},
   "source": [
    "<i>LASER, NepBERTa embeddings that are being considered should support embeddings generation through the script, add the functionality in the script </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0fa8d-80f7-409a-9d9e-de008130218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the dataset to vectors generated by pre-trained Nepali word2vec\n",
    "\n",
    "from generate_embeddings import load_word2vec_model, generate_word2vec_embeddings\n",
    "\n",
    "# Loading the model with pre-trained embeddings\n",
    "pre_trained_word2vec = load_word2vec_model(\"../Data/Embeddings/nepali_embeddings_word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f66c38-d175-49b2-8da9-216206cb8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating a new df and generating embeddings for text data \n",
    "\n",
    "df_pretrained_word2vec = df_copy.copy()\n",
    "df_pretrained_word2vec= generate_word2vec_embeddings(df_pretrained_word2vec, \"Text\", pre_trained_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba0c85-cea3-4050-844b-49087b92df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the df as csv inside /Data/Preprocessed for ready to use df while classifiers training \n",
    "\n",
    "df_pretrained_word2vec.drop(\"Text\", axis=1, inplace= True)\n",
    "df_pretrained_word2vec.to_csv(\"../Data/Preprocessed/pretrained_word2vec_df.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179703dd-6e20-479b-9b33-4a60ce0b49f6",
   "metadata": {},
   "source": [
    "Loading and Generate embedding for our own word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf706a2f-d8f4-45a3-9248-bfaf8639711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from generate_embeddings import load_word2vec_model, generate_word2vec_embeddings\n",
    "\n",
    "word2vec = load_word2vec_model(\"../Data/Embeddings/word2vec_embeddings.txt\")\n",
    "\n",
    "df_word2vec= df_copy.copy()\n",
    "df_word2vec = generate_word2vec_embeddings(df_word2vec, \"Text\", word2vec)\n",
    "\n",
    "df_word2vec.drop(\"Text\", axis=1, inplace= True)\n",
    "df_word2vec.to_csv(\"../Data/Preprocessed/word2vec_df.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5f418-897f-4434-b521-611d0c641be9",
   "metadata": {},
   "source": [
    "## Fine-tuning pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5cdbc89-36c0-47fa-af77-e474415f92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_embeddings import load_word2vec_model, generate_word2vec_embeddings\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# Loading the model with pre-trained embeddings\n",
    "finetune_word2vec = load_word2vec_model(\"../Data/Embeddings/nepali_embeddings_word2vec.txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83296fa2-1f69-483c-ab69-06ce597dc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_word2vec_model returns keyedVectors objects which needs to converted back to Word2Vec\n",
    "\n",
    "finetune_word2vec_model = Word2Vec(vector_size=finetune_word2vec.vector_size, min_count=1)\n",
    "finetune_word2vec_model.build_vocab([list(finetune_word2vec.key_to_index.keys())], update= False)  \n",
    "\n",
    "finetune_word2vec_model.wv.vectors= finetune_word2vec.vectors\n",
    "finetune_word2vec_model.wv.key_to_index = finetune_word2vec.key_to_index\n",
    "finetune_word2vec_model.wv.index_to_key = finetune_word2vec.index_to_key\n",
    "\n",
    "# initialize vectors_lockf for training\n",
    "finetune_word2vec_model.vectors_lockf =  np.ones(len(finetune_word2vec_model.wv), dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0a4c75-7d4d-48d1-b725-893846706cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/angel-\n",
      "[nltk_data]     tamang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/angel-\n",
      "[nltk_data]     tamang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...</td>\n",
       "      <td>2</td>\n",
       "      <td>[गुठी, विधेक, ल्याएर, ठमेल, मा, राज, गुठि, को,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[दले, ले, देश, सकेछन, सबै, बेचे, र, खान, सुरू,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !</td>\n",
       "      <td>3</td>\n",
       "      <td>[नेपाल, को, ससकृती, ध्वस्त, पार्ने, योजना, हो,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...</td>\n",
       "      <td>3</td>\n",
       "      <td>[मठ, मन्दिर, गुम्बा, का, जग्गा, हरु, मा, भुमाफ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[नेपाल, का, कल, कर्खाना, र, नदि, नाला, बेची, स...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Target  \\\n",
       "0  गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...       2   \n",
       "1  दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...       3   \n",
       "2        नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !       3   \n",
       "3  मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...       3   \n",
       "4  नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...       3   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [गुठी, विधेक, ल्याएर, ठमेल, मा, राज, गुठि, को,...  \n",
       "1  [दले, ले, देश, सकेछन, सबै, बेचे, र, खान, सुरू,...  \n",
       "2  [नेपाल, को, ससकृती, ध्वस्त, पार्ने, योजना, हो,...  \n",
       "3  [मठ, मन्दिर, गुम्बा, का, जग्गा, हरु, मा, भुमाफ...  \n",
       "4  [नेपाल, का, कल, कर्खाना, र, नदि, नाला, बेची, स...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing using nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df_finetune_word2vec= df_copy.copy()\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Remove Nepali stop words\n",
    "    # tokens = [token for token in tokens if token not in nepali_stopwords]\n",
    "    # Apply stemming\n",
    "    #tokens = [nepali_stemmer.stemWord(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "df_finetune_word2vec['Tokens']= df_finetune_word2vec['Text'].apply(preprocess_text)\n",
    "df_finetune_word2vec.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea33c369-51ff-40b4-86d2-57b65a1aa0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_finetune_word2vec['Tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ee7e92-dcc2-4822-88d6-19fbc6d722eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating vocalbulary and fine-tuning the model\n",
    "\n",
    "finetune_word2vec_model.build_vocab(sentences, update= True)  # update True to handle out of vocab words in pre-trained model\n",
    "finetune_word2vec_model.train(sentences, total_examples=len(sentences), epochs=5)\n",
    "\n",
    "# Saving the model keyedVectors\n",
    "\n",
    "finetune_word2vec_model.wv.save_word2vec_format(\"../Data/Embeddings/fintuned_word2vec_embeddings.txt\", binary= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45356cc0-1e39-45fa-8215-c53ec8147d41",
   "metadata": {},
   "source": [
    "Actually did a typo up there, while saving embeddings. I will fix manually, moving on :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6312110d-2f30-4a94-b763-8889edd75375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we resume to the original intention of this notebook, i.e. generating embeddings for the dataset\n",
    "\n",
    "ft_word2vec = load_word2vec_model(\"../Data/Embeddings/finetuned_word2vec_embeddings.txt\")\n",
    "\n",
    "df_finetune_word2vec.drop(\"Tokens\", axis=1, inplace= True)\n",
    "\n",
    "df_ft_word2vec= generate_word2vec_embeddings(df_finetune_word2vec, \"Text\", ft_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e663c8a-e1b4-4151-9400-8752ae1f9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_word2vec.drop(\"Text\", axis=1, inplace= True)\n",
    "df_ft_word2vec.to_csv(\"../Data/Preprocessed/finetuned_word2vec_df.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7865c4-5c05-49c6-afeb-f71955cd16ef",
   "metadata": {},
   "source": [
    "# Embeddings for new unbalanced raw dataset with finetuned word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "788b8298-34c7-48a8-84a3-db1196f08899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2859 entries, 0 to 2858\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Text         2859 non-null   object\n",
      " 1   Aspect Term  2859 non-null   object\n",
      " 2   Sentiment    2859 non-null   object\n",
      " 3   Polarity     2859 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 89.5+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_unbal= pd.read_csv(\"../Data/Raw/Nepali_dataset_unbal.csv\")\n",
    "df_unbal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ebccf3f-3620-4675-b748-a976924cf790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment  Polarity\n",
       "GENERAL    1           1358\n",
       "           0            901\n",
       "PROFANITY  0            250\n",
       "VIOLENCE   1            159\n",
       "           0            108\n",
       "PROFANITY  1             83\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unbal.loc[:,\"Sentiment\":\"Polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "432b93e8-a485-4190-bbf8-1d80614cc5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Aspect Term</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...</td>\n",
       "      <td>जोगाउन को लागि</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...</td>\n",
       "      <td>लखेटनु पछ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !</td>\n",
       "      <td>ससकृती ध्वस्त पार्ने</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...</td>\n",
       "      <td>भुमाफिया</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...</td>\n",
       "      <td>बेची सके</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Aspect Term  \\\n",
       "0  गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...        जोगाउन को लागि   \n",
       "1  दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...             लखेटनु पछ   \n",
       "2        नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !  ससकृती ध्वस्त पार्ने   \n",
       "3  मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...              भुमाफिया   \n",
       "4  नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...              बेची सके   \n",
       "\n",
       "   Target  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing custom python script to transform Hate sentiment dataset's Output classes to single target class\n",
    "from create_target_label import create_target_column\n",
    "\n",
    "df_unbal,label_encoder = create_target_column(df_unbal, \"Sentiment\", \"Polarity\")\n",
    "df_unbal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56c4b07f-c894-4c19-8fcc-1e1bf7926444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GENERAL_0',\n",
       " 'GENERAL_1',\n",
       " 'PROFANITY_0',\n",
       " 'PROFANITY_1',\n",
       " 'VIOLENCE_0',\n",
       " 'VIOLENCE_1']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "701b689c-0b78-49c9-8e6b-1e90b25bf676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERAL_0: 0\n",
      "GENERAL_1: 1\n",
      "PROFANITY_0: 2\n",
      "PROFANITY_1: 3\n",
      "VIOLENCE_0: 4\n",
      "VIOLENCE_1: 5\n"
     ]
    }
   ],
   "source": [
    "# The classes and there mapping are\n",
    "\n",
    "for class_name, encoded_value in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"{class_name}: {encoded_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd7e0e51-53cb-4793-ae94-7c82ec8f7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unbal.drop(\"Aspect Term\", axis=1, inplace=True)\n",
    "df_unbal_word2vec= generate_word2vec_embeddings(df_unbal, \"Text\", ft_word2vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea7f0cec-951b-4d29-81f5-f2ee799df58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unbal_word2vec.drop(\"Text\", axis=1, inplace= True)\n",
    "df_unbal_word2vec.to_csv(\"../Data/Preprocessed/unbal_word2vec.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ed9a4-dac5-46fe-a7ef-b602bd9ad9f5",
   "metadata": {},
   "source": [
    "# Remove stopwords from this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7e59b35-edfd-413d-848c-8e3db4aed844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Target\n",
       "0  गुठी विधेक ल्याएर ठमेल मा राज गुठि को जग्गा मा...       0\n",
       "1  दले ले देश सकेछन सबै बेचे र खान सुरू गरेछन अब ...       1\n",
       "2        नेपाल को ससकृती ध्वस्त पार्ने योजना हो यो !       1\n",
       "3  मठ मन्दिर गुम्बा का जग्गा हरु मा भुमाफिया को न...       1\n",
       "4  नेपाल का कल कर्खाना र नदि नाला बेची सके अब मठ ...       1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stopword= df_unbal.copy()\n",
    "df_stopword.drop(\"word2vec_embeddings\", axis=1, inplace= True)\n",
    "df_stopword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec62f8ed-4dec-426a-95a3-ae7f8f60355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/angel-\n",
      "[nltk_data]     tamang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/angel-\n",
      "[nltk_data]     tamang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nepali_stopwords = stopwords.words('nepali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75221a55-1455-44aa-9754-1d721022f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing: remove punctuation, stopwords removal and lowering to the 'Text' column\n",
    "df_stopword['Text'] = (\n",
    "    df_stopword['Text']\n",
    "    .str.lower()               # Convert to lowercase\n",
    "    .str.replace(f\"[{string.punctuation}]\", \"\")  # Remove punctuation\n",
    "    .apply(lambda x: ' '.join([word for word in x.split() if word not in nepali_stopwords]))  # Remove Nepali stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d2982d7-bbd5-4e39-830b-9b5b08d30ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गुठी विधेक ल्याएर ठमेल राज गुठि जग्गा छाया सेन...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>दले देश सकेछन बेचे खान सुरू गरेछन दले लखेटनु पछ ।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>नेपाल ससकृती ध्वस्त पार्ने योजना !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>मठ मन्दिर गुम्बा जग्गा हरु भुमाफिया नजर परे हु...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>नेपाल कल कर्खाना नदि नाला बेची सके मठ मन्दीर ब...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Target\n",
       "0  गुठी विधेक ल्याएर ठमेल राज गुठि जग्गा छाया सेन...       0\n",
       "1  दले देश सकेछन बेचे खान सुरू गरेछन दले लखेटनु पछ ।       1\n",
       "2                 नेपाल ससकृती ध्वस्त पार्ने योजना !       1\n",
       "3  मठ मन्दिर गुम्बा जग्गा हरु भुमाफिया नजर परे हु...       1\n",
       "4  नेपाल कल कर्खाना नदि नाला बेची सके मठ मन्दीर ब...       1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stopword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edf74461-7e9f-4fb3-b952-649f3020ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since tokenization, flattening is handled by our custom script, we'll do that through word2vec\n",
    "\n",
    "df_stopword_word2vec= generate_word2vec_embeddings(df_stopword, \"Text\", ft_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e97c6412-23e3-490b-bd32-8fb47a095632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>गुठी विधेक ल्याएर ठमेल राज गुठि जग्गा छाया सेन...</td>\n",
       "      <td>-0.397011</td>\n",
       "      <td>-0.332576</td>\n",
       "      <td>-0.236242</td>\n",
       "      <td>0.253269</td>\n",
       "      <td>0.419496</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>-0.523131</td>\n",
       "      <td>0.053369</td>\n",
       "      <td>-0.241984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.792762</td>\n",
       "      <td>0.933628</td>\n",
       "      <td>-0.525954</td>\n",
       "      <td>-0.087645</td>\n",
       "      <td>-0.115768</td>\n",
       "      <td>0.036328</td>\n",
       "      <td>0.341761</td>\n",
       "      <td>0.148474</td>\n",
       "      <td>-0.50575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text         0         1  \\\n",
       "0  गुठी विधेक ल्याएर ठमेल राज गुठि जग्गा छाया सेन... -0.397011 -0.332576   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0 -0.236242  0.253269  0.419496  0.379075 -0.523131  0.053369 -0.241984  ...   \n",
       "\n",
       "        291       292       293       294       295       296       297  \\\n",
       "0 -0.792762  0.933628 -0.525954 -0.087645 -0.115768  0.036328  0.341761   \n",
       "\n",
       "        298      299  Target  \n",
       "0  0.148474 -0.50575       0  \n",
       "\n",
       "[1 rows x 302 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stopword_word2vec.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2089a0d-3dfe-4850-a50c-3dd83f2c8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stopword_word2vec.drop(\"Text\", axis=1, inplace= True)\n",
    "df_stopword_word2vec.to_csv(\"../Data/Preprocessed/unbal_word2vec.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4829b1-f40c-43d5-aebe-43c787c6022e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
